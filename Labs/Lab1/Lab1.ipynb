{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 1\n",
    "## **Text processing**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1:\n",
    "Benchmark different language-detection algorithm by computing accuracy of each approach.\n",
    "- FastText\n",
    "- LangID\n",
    "- langDetect\n",
    "\n",
    "Hint: use language code conversion `iso639-lang`\n",
    "\n",
    "Report\n",
    "- Accuracy\n",
    "- Average time per example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/langid_dataset.csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### My code.."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "\n",
    "# read file\n",
    "corpus = pd.read_csv('langid_dataset.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sneak peak\n",
    "corpus.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# accuracy function and format printing-function\n",
    "\n",
    "def acc(clf_fcn, corpus):\n",
    "    correct = 0\n",
    "    for text,lan in zip(corpus.Text, corpus.language):\n",
    "        try:\n",
    "            pred = clf_fcn(text)\n",
    "            if type(pred) == tuple: # langid_classify returns lang and prob\n",
    "                pred = pred[0]\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            correct += (pred[:2] == Lang(lan).pt1)*1 #due to 'zh-hans'\n",
    "    return correct\n",
    "\n",
    "def output(method, corpus, correct, elapsed):\n",
    "    print(f'{method:s} \\nAccuracy: {correct/len(corpus):.3f}. Est time/sample: {elapsed/len(corpus)*1000:.3f} ms')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "#!pip install iso639-lang\n",
    "from iso639 import Lang"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FastText (fastlangid)\n",
    "from fastlangid import LID\n",
    "\n",
    "method = 'FastText'\n",
    "\n",
    "# fastText model\n",
    "fastText_clf = LID()\n",
    "\n",
    "# accuracy and timing\n",
    "start = time.time()\n",
    "correct = acc(fastText_clf.predict, corpus)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# print result\n",
    "output('FastText', corpus, correct, elapsed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LangID\n",
    "#!pip install langid\n",
    "import langid\n",
    "\n",
    "method = 'LangID'\n",
    "\n",
    "# accuracy and timing\n",
    "start = time.time()\n",
    "correct = acc(langid.classify, corpus)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# print result\n",
    "output(method, corpus,correct, elapsed)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# langdetect\n",
    "#!pip install langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "method = 'langdetect'\n",
    "\n",
    "# accuracy and timing\n",
    "start = time.time()\n",
    "correct = acc(detect, corpus)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# print result\n",
    "output(method, corpus, correct, elapsed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2\n",
    "For English-written text, apply word-level tokenization. What is the average number of words per sentence?\n",
    "Implement word-tokenization using both nltk and spacy. Report the results for both of them.\n",
    "For spaCy use the en_core_web_sm model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### My code..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# find only english texts\n",
    "corpus_eng = corpus.loc[corpus.language=='English']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imports\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "#!pip install -U spacy\n",
    "#python -m spacy download en_core_web_sm (run in terminal)\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# counting average number of words\n",
    "\n",
    "# nltk\n",
    "tot_words = 0\n",
    "for sentence in corpus_eng.Text:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tot_words += len(tokens)\n",
    "\n",
    "print(f'NLTK \\nAverage number of words/sentence: {tot_words/len(corpus_eng):.2f}')\n",
    "\n",
    "# spacy\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\", disable = ['parser','ner','tagger', 'attribute_ruler', 'lemmatizer']) # only using 'tok2vec'\n",
    "tot_words = 0\n",
    "for sentence in corpus_eng.Text:\n",
    "    doc = spacy_nlp(sentence)\n",
    "    sentence_words = 0\n",
    "    sentence_words += sum([1 for w in doc]) #includes \"space\" and \" - \"\n",
    "    tot_words += sentence_words\n",
    "\n",
    "print(f'spaCy \\nAverage number of words/sentence: {tot_words/len(corpus_eng):.2f}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spacy_nlp.pipe_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3\n",
    "\n",
    "Dependency Parsing aims at analyzing the grammatical structure of sentences. The main goal is to find out related words as well as the type of the relationship between them.\n",
    "The output of this step is a dependency tree similar to the one reported in the figure below.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use spacy to parse the dependency tree of a **randomly selected** sentence. You can both use English sentences or your native language (if supported in [spaCy](https://spacy.io/usage/models/)). Use [displaCy](https://explosion.ai/demos/displacy) to visualize the result in the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus_eng"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# choose random sentence\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence = corpus_eng.sample(random_state=123).Text.iloc[0]\n",
    "doc = nlp(sentence)\n",
    "spacy.displacy.render(doc, style=\"dep\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 4\n",
    "For the same sentence selected in the previous step apply all the following steps:\n",
    "1. Lemmatization: convert each word to its root form.\n",
    "2. Stopword removal: remove language-specific stopwords.\n",
    "3. Part of Speech Tagging: for each word in the sentence display its part-of-speech.\n",
    "\n",
    "For each step, print the resulting list on the console."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# what is included in pipeline\n",
    "print(f'{nlp.pipe_names}\\n')\n",
    "\n",
    "# original sentence\n",
    "print(f'Original sentence: \\n{sentence}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. lemmatization\n",
    "doc = nlp(sentence)\n",
    "sentence_lemma = \" \".join([token.lemma_ for token in doc])\n",
    "print(f'After lemmatization: \\n{sentence_lemma}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. stopword removal\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "sentence_wo_sw = \" \".join([w for w in sentence_lemma.split() if w not in stopwords])\n",
    "\n",
    "print(f'After removal of stopwords: \\n{sentence_wo_sw}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. POS\n",
    "\n",
    "# use latest version of sentence in doc\n",
    "doc = nlp(sentence_wo_sw)\n",
    "for token in doc:\n",
    "    print(f'{token.text:{15}}, {token.pos_}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Occurrence-based text representation - TF-IDF**\n",
    "\n",
    "---\n",
    "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. It allows to create occurrence-based vector representation for each document.\n",
    "\n",
    "## Exercise 5\n",
    "Use TF-IDF to vectorize each sentence in the original data collection. You can choose your preferred implementation for TF-IDF vectorization. It is also available on [SciKit-Learn library](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!pip install sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "v = TfidfVectorizer()\n",
    "X = v.fit_transform(corpus.Text)\n",
    "#X = X.toarray()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_names = v.get_feature_names_out()\n",
    "\n",
    "#dense = X.todense()\n",
    "\n",
    "#arr = X.toarray()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 6\n",
    "Build a supervised multi-class language detector using as features the vector obtained by TF-IDF representation. Use 80% of the data to train the language detector and 20% of the data for assessing its accuracy.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "target = corpus.language.values # yields numpy.array\n",
    "text = corpus.Text.values\n",
    "\n",
    "#X_tr, X_test, y_tr, y_test = train_test_split(X, target, test_size = 0.2, random_state=42, stratify=target)\n",
    "\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(text, target, test_size = 0.2, random_state=42, stratify=target)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=0.2, random_state=42, stratify=y_tr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# apply tf-idf on input variables (text)\n",
    "X_train_tfidf = v.fit_transform(X_train) # only fit to the training data\n",
    "X_val_tfidf = v.transform(X_val)\n",
    "X_test_tfidf = v.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# one-vs-rest classifier using logistic regression\n",
    "clf_lr = OneVsRestClassifier(LogisticRegression(penalty='l2')).fit(X_train_tfidf, y_train)\n",
    "\n",
    "# multinomial naive Bayes\n",
    "clf_mnb = MultinomialNB(alpha=0.5).fit(X_train_tfidf, y_train)\n",
    "\n",
    "# training performance\n",
    "print(f'One-vs-rest logistic regression \\nTraining acc: {clf_lr.score(X_train_tfidf, y_train):.3f} \\nValidation acc: {clf_lr.score(X_val_tfidf, y_val):.3f}')\n",
    "\n",
    "print(f'Multinomial Naive Bayes \\nTraining acc: {clf_mnb.score(X_train_tfidf, y_train):.3f} \\nValidation acc: {clf_mnb.score(X_val_tfidf, y_val):.3f}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test performance\n",
    "print(f'One-vs-rest logistic regression \\nTest acc: {clf_lr.score(X_test_tfidf, y_test):.3f}')\n",
    "print(f'Multinomial Naive Bayes \\nTest acc: {clf_mnb.score(X_test_tfidf, y_test):.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Topic Modelling**\n",
    "\n",
    "Occurrence-based representations are high-dimensional, what is the dimension of the generated TF-IDF vector representation?\n",
    "Topic modelling focuses on caturing latent topics in large document corpora.\n",
    "\n",
    "The data collection used in this second part of the practice is provided [here](https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/CovidFake_filtered.csv) - [source: Zenodo](https://zenodo.org/record/4282522#.YVdCXcbOOpd)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dimensions of generated TF-IDF for whole corpus\n",
    "np.shape(X) #277.719 features (tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 7\n",
    "\n",
    "Latent Semantic Indexing (LSI) models underlying concepts by using SVD (Singular Value Decomposition).\n",
    "\n",
    "Use [gensim](https://radimrehurek.com/gensim/) library to:\n",
    "1. Create a corpus composed of the headlines contained in the data collection.\n",
    "2. Generate a [dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html) to create a word -> id mapping (required by LSI module).\n",
    "3. Using the dictionary, preprocess the corpus to obtain the representation required for LSI model training ([documentation here](https://radimrehurek.com/gensim/models/lsimodel.html)).\n",
    "4. Inspect the top-5 topics generated by the LSI model for the analysed corpus."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/CovidFake_filtered.csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create corpus of all words in headlines\n",
    "\n",
    "# read file\n",
    "corpus_covid = pd.read_csv('CovidFake_filtered.csv').headlines.values\n",
    "\n",
    "# merge all headlines and split each word\n",
    "corpus_covid_merged = [sentence.split() for sentence in corpus_covid]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus_covid[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. dictionary for ID-mapping\n",
    "from gensim.corpora import Dictionary\n",
    "dct = Dictionary(corpus_covid_merged)  # initialize a Dictionary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Preprocess corpus to get representation for LSI-model, using dictionary\n",
    "processed_corpus = [dct.doc2bow(text) for text in corpus_covid_merged]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Train the LSI-model\n",
    "#!pip install gensim\n",
    "from gensim.models import LsiModel\n",
    "model = LsiModel(processed_corpus, id2word=dct)\n",
    "model.print_topics(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 8 (Optional)\n",
    "\n",
    "The top-scored words contributing to each topic (if no stopword removal is applied) are english common words (e.g., *to, for, in, of, on*..). Moreover, missing punctuation removal could be critical for topic identification. Repeat the same procedure of Ex. 7 by adding preliminary preprocessing step to:\n",
    "1. **remove stopwords**\n",
    "2. **strip punctuation**\n",
    "3. **lowercase all words**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove stopwords, punctuation and make lower case\n",
    "\n",
    "#from nltk.corpus import stopwords # not working\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# nltk.download('stopwords') # not working\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "corpus_covid_no_sw = [[w.lower() for w in s if w.lower() not in stopwords] for s in corpus_covid_merged]\n",
    "corpus_covid_no_sw = [[w.translate(str.maketrans('', '', string.punctuation)) for w in s] for s in corpus_covid_no_sw]\n",
    "print(corpus_covid_no_sw[:10])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rs_tm_dict = Dictionary(corpus_covid_merged)\n",
    "rs_processed_corpus = [rs_tm_dict.doc2bow(text) for text in corpus_covid_no_sw]\n",
    "rs_model = LsiModel(rs_processed_corpus, id2word=rs_tm_dict)\n",
    "rs_model.print_topics(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 9 (Optional)\n",
    "\n",
    "Leveraging the same corpus used for LSI model generation, apply LDA modelling setting the number of topics to 5. Display the words most contributing to the those topics according to the LDA model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "lda = LdaModel(rs_processed_corpus, id2word=rs_tm_dict, num_topics=3)\n",
    "lda.print_topics(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 10 (Optional)\n",
    "\n",
    "Using [pyLDAvis]() library build an interactive visualization for the trained LDA model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "lda_display = gensimvis.prepare(lda, rs_processed_corpus, rs_tm_dict, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
